#!crnn/rnn.py
# kate: syntax python;

# via:
# /u/irie/setups/switchboard/2018-02-13--end2end-zeyer/config-train/bpe_1k.multihead-mlp-h1.red8.enc6l.encdrop03.decbs.ls01.pretrain2.nbd07.config
# Kazuki BPE1k baseline, from Interspeech paper.

import os
import numpy
from subprocess import check_output, CalledProcessError
from Pretrain import WrapEpochValue

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if debug_mode or check_output(["hostname"]).strip().decode("utf8") in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    try:
        cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    except CalledProcessError:
        print("Cache manager: Error occured, using local file")
        return filename
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn

# data
num_inputs = 40  # Gammatone 40-dim
num_outputs = {"bpe": (1030, 1), "data": (num_inputs, 2)}  # see vocab
EpochSplit = 6

def get_sprint_dataset(data):
    assert data in {"train", "cv", "dev", "hub5e_01", "rt03s"}
    epoch_split = {"train": EpochSplit}.get(data, 1)
    corpus_name = {"cv": "train"}.get(data, data)  # train, dev, hub5e_01, rt03s

    # see /u/tuske/work/ASR/switchboard/corpus/readme
    # and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
    files = {}
    files["config"] = "config/training.config"
    files["corpus"] = "/work/asr3/irie/data/switchboard/corpora/%s.corpus.gz" % corpus_name
    if data in {"train", "cv"}:
        files["segments"] = "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000"}[data]
    files["features"] = "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.%s.bundle" % corpus_name
    for k, v in sorted(files.items()):
        assert os.path.exists(v), "%s %r does not exist" % (k, v)
    estimated_num_seqs = {"train": 227047, "cv": 3000}  # wc -l segment-file

    args = [
        "--config=" + files["config"],
        lambda: "--*.corpus.file=" + cf(files["corpus"]),
        lambda: "--*.corpus.segments.file=" + (cf(files["segments"]) if "segments" in files else ""),
        "--*.corpus.segment-order-shuffle=true",
        "--*.segment-order-sort-by-time-length=true",
        "--*.segment-order-sort-by-time-length-chunk-size=%i" % {"train": (EpochSplit or 1) * 1000}.get(data, -1),
        lambda: "--*.feature-cache-path=" + cf(files["features"]),
        "--*.log-channel.file=/dev/null",
        "--*.window-size=1",
    ]
    d = {
        "class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
        "sprintConfigStr": args,
        "partitionEpoch": epoch_split,
        "estimated_num_seqs": (estimated_num_seqs[data] // epoch_split) if data in estimated_num_seqs else None,
    }
    d.update(sprint_interface_dataset_opts)
    return d

sprint_interface_dataset_opts = {
    "input_stddev": 3.,
    "bpe": {
        'bpe_file': '/work/asr3/irie/data/switchboard/subword_clean/ready/swbd_clean.bpe_code_1k',
        'vocab_file': '/work/asr3/irie/data/switchboard/subword_clean/ready/vocab.swbd_clean.bpe_code_1k',
        'seq_postfix': [0]
    }}

train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
EncKeyTotalDim = 1024
AttNumHeads = 1
EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads
EncValueTotalDim = 2048
EncValuePerHeadDim = EncValueTotalDim // AttNumHeads
LstmDim = EncValueTotalDim // 2

class RNMTPlus:

    def __init__(self, enc_N=6, dec_N=8):
        self.enc_N = enc_N
        self.dec_N = dec_N

        self.EmbDim = 1024 # embedding dimension
        self.LstmDim = 1024
        self.EncProjDim = 1024

        self.EncKeyTotalDim = 1024
        self.AttNumHeads = 4
        self.EncKeyPerHeadDim = self.EncKeyTotalDim // self.AttNumHeads
        self.EncValueTotalDim = 1024
        self.EncValuePerHeadDim = self.EncValueTotalDim // self.AttNumHeads

        self.emb_dropout = 0.3
        self.ff_dropout = 0.3
        self.att_dropout = 0.3

        self.label_smoothing = 0.1

        self.res_start = 3
  
        # apply per gate norm only
        # https://github.com/tensorflow/lingvo/blob/master/lingvo/core/rnn_cell.py#L1296
        self.unit_opts = {
          'global_norm': False, 
          'global_norm_joined': True,
          'cell_norm': False
        }

        self.pool_size = [2, 2, 2, 1, 1]
        assert len(self.pool_size) == enc_N - 1

    def add_enc_layer(self, d, inp, output, idx, res_conn):
        # 1. apply dropout on input (already done in the build_network function)
        # 2. BiRNN with per gate norm lstm cells and concat the result
        # 3. Apply dropout / res connection

        d[output + '_lstm_fw'] = {"class": "rec", "unit": "layernormvariantslstm", "unit_opts": self.unit_opts,
                                  "n_out": self.LstmDim, "direction": 1, "from": [inp]}
        d[output + '_lstm_bw'] = {"class": "rec", "unit": "layernormvariantslstm", "unit_opts": self.unit_opts,
                                  "n_out": self.LstmDim, "direction": -1, "from": [inp]}
       
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_lstm_fw', output + '_lstm_bw'], "dropout": self.ff_dropout}

        if res_conn:
            d[output + '_pre_pool'] = {"class": "combine", "kind": "add", "from": [inp, output + '_ff_drop']}
        else:
            d[output + '_pre_pool'] = {"class": "copy", "from": [output + '_ff_drop']}
  
        if idx < len(self.pool_size):
          d[output] = {"class": "pool", "mode": "max", "padding": "same", "pool_size": (self.pool_size[idx],), 
                       "from": [output + "_pre_pool"], "trainable": False}
        else:
          d[output] = {"class": "copy", "from": [output + "_pre_pool"]}

    def add_dec_first_layer(self, d, output):
        # t: encoder step
        # i: decoder step

        # Here we compute the attention context vector using multi-head attention
        # 1. Apply lstm cell using context vector
        # 2. Concat the result with the context vector
        # 3. Apply again lstm cell with dropout on its output and res connection
        
        d["weight_feedback"] = {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": self.EncKeyTotalDim}
        
        d["s_transformed"] = {"class": "linear", "activation": None, "with_bias": False, "from": ["s"], "n_out": self.EncKeyTotalDim}
  
        d['energy_in'] = {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "s_transformed"], # [s_{i-1}, h_t]
                          "n_out": self.EncKeyTotalDim} # (B, enc-T, D)

        d['energy_tanh'] = {"class": "activation", "activation": "tanh", "from": ["energy_in"]}

        d['energy'] = {"class": "linear", "activation": None, "with_bias": False, "from": ['energy_tanh'],
                       "n_out": self.AttNumHeads} # (B, enc-T, H, 1)

        d['att_weights'] = {"class": "softmax_over_spatial", "from": ["energy"]}  # (B, enc-T, H, 1)
        
        d["accum_att_weights"] = {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:inv_fertility"],
                                  "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": self.AttNumHeads, "shape": (None, self.AttNumHeads)}}

        d['att0'] = {"class": "generic_attention", "weights": "att_weights", "base": "base:enc_value"}  # (B, H, V)

        d['att1'] = {"class": "merge_dims", "axes": "except_batch", "from": ["att0"]}  # (B, H*V)

        d['att'] = {"class": "dropout", "from": ["att1"], "dropout": self.att_dropout} # apply attention dropout
          
        # RNN(s_{i-1}, y_{i-1}, c_{i-1})
        d['s'] = {"class": "rnn_cell", "unit": "layernormvariantslstm", "unit_opts": self.unit_opts, 
                  "from": ["prev:target_embed", "prev:att"], "n_out": self.LstmDim}
        
        d[output] = {"class": "copy", "from": ["s"]} # s_i
    
    def add_dec_layer(self, d, inp, output, res_conn):
        d[output + '_inp_concat'] = {"class": "copy", "from": [inp, 'att']} # use current attention context for all s_i where i > 0
        
        d[output + '_s'] = {"class": "rnn_cell", "unit": "layernormvariantslstm", "unit_opts": self.unit_opts,
                            "from": [output + '_inp_concat'], "n_out": self.LstmDim}
    
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_s'], "dropout": self.ff_dropout}

        if res_conn:
            d[output + '_add'] = {"class": "combine", "kind": "add", "from": [inp, output + '_ff_drop']}

            d[output] = {"class": "copy", "from": [output + '_add']}
        else:
            d[output] = {"class": "copy", "from": [output + '_ff_drop']}

    def build_network(self):
        network = {
            "source_embed_raw": {"class": "linear", "activation": None, "with_bias": False, "n_out": self.EmbDim},

            "source_embed_drop": {"class": "dropout", "from": ["source_embed_raw"], "dropout": self.emb_dropout},

            "source_embed": {"class": "copy", "from": ["source_embed_drop"]},

            # add encoder layers later through a separate method
  
            "encoder": {"class": "copy", "from": ["enc_%d" % self.enc_N]}, # dim: 2*LstmDim

            "enc_ctx": {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                        "n_out": self.EncKeyTotalDim},  # (B, enc-T, D)

            "inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": self.AttNumHeads},

            "enc_value": {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncValuePerHeadDim),
                          "from": ["enc_ctx"]},  # (B, enc-T, H, D'/H)

            "output": {"class": "rec", "from": [], "unit": {
                'output': {'class': 'choice', 'target': 'bpe', 'beam_size': beam_size, 'from': ["output_prob"]},
                "end": {"class": "compare", "from": ["output"], "value": 0},
                "target_embed_raw": {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'],
                                     "n_out": self.EmbDim, "initial_output": 0},
                "target_embed": {"class": "dropout", "from": ["target_embed_raw"], "dropout": self.emb_dropout},

                # add decoder layers later through a separate method

                "decoder": {"class": "copy", "from": ["dec_%i" % self.dec_N]},
        
                # P(y_i|y_{<i}, x_1^T) = softmax(s_i, y_{i-1}, c_i)
                "readout_in": {"class": "linear", "from": ["decoder", "prev:target_embed", "att"], "activation": None, "n_out": self.LstmDim},  # merge + post_merge bias
                
                "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"]},
                
                "output_prob": {
                  "class": "softmax", "from": ["readout"], "dropout": 0.3,
                  "target": "bpe", "loss": "ce", "loss_opts": {"label_smoothing": self.label_smoothing}
                }
            }, "target": "bpe", "max_seq_len": "max_len_from('base:encoder')"},

            "decision": {
                "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "bpe",
                "loss_opts": {
                    #"debug_print": True
                }
            }
        }

        # Add encoder layers
        self.add_enc_layer(network, "source_embed", "enc_1", 0, False)
        for n in range(1, self.enc_N):
            self.add_enc_layer(network, "enc_%i" % n, "enc_%i" % (n+1), n, (n+1) >= self.res_start)
            
        # Add decoder layers
        net_out_unit = network["output"]["unit"]
        self.add_dec_first_layer(net_out_unit, "dec_1")
        for n in range(1, self.dec_N):
            self.add_dec_layer(net_out_unit, "dec_%i" % n, "dec_%i" % (n+1), (n+1) >= self.res_start)
        
        return network

network = RNMTPlus().build_network()

search_output_layer = "decision"
target = "bpe"
debug_print_layer_output_template = True

# trainer
batching = "random"
# Seq-length 'data' Stats:
#  37867 seqs
#  Mean: 447.397258827
#  Std dev: 350.353162012
#  Min/max: 15 / 2103
# Seq-length 'bpe' Stats:
#  37867 seqs
#  Mean: 14.1077719386
#  Std dev: 13.3402518828
#  Min/max: 2 / 82
# With batch_size 30k, max_seqs 200, max_seq_length {bpe:75}, I get GPU mem ~8.9GB.
log_batch_size = True
batch_size = 10000
max_seqs = 200
max_seq_length = {"bpe": 75}
#chunking = ""  # no chunking
truncation = -1

def custom_construction_algo(idx, net_dict):
    # For debugging, use: python3 ./crnn/Pretrain.py config... Maybe set repetitions=1 below.
    # We will first construct layer-by-layer, starting with 2 layers.
    # Initially, we will use a higher reduction factor, and at the end, we will reduce it.
    # Also, we will initially have not label smoothing.
    orig_num_lstm_layers = 0
    while "lstm%i_fw" % orig_num_lstm_layers in net_dict:
        orig_num_lstm_layers += 1
    assert orig_num_lstm_layers >= 2
    orig_red_factor = 1
    for i in range(orig_num_lstm_layers - 1):
        orig_red_factor *= net_dict["lstm%i_pool" % i]["pool_size"][0]
    num_lstm_layers = idx + 4  # idx starts at 0. start with 4 layers
    if idx == 0:
        net_dict["lstm%i_fw" % (orig_num_lstm_layers - 1)]["dropout"] = 0
        net_dict["lstm%i_bw" % (orig_num_lstm_layers - 1)]["dropout"] = 0
    if idx >= 1:
        num_lstm_layers -= 1  # repeat like idx=0, but now with dropout
    if num_lstm_layers > orig_num_lstm_layers:
        return None
    # Use label smoothing only at the very end.
    net_dict["output"]["unit"]["output_prob"]["loss_opts"]["label_smoothing"] = 0
    # Leave the last lstm layer as-is, but only modify its source.
    net_dict["lstm%i_fw" % (orig_num_lstm_layers - 1)]["from"] = ["lstm%i_pool" % (num_lstm_layers - 2)]
    net_dict["lstm%i_bw" % (orig_num_lstm_layers - 1)]["from"] = ["lstm%i_pool" % (num_lstm_layers - 2)]
    # Fix reduction factor
    f = 1
    for i in range(num_lstm_layers - 2):
        if f >= orig_red_factor:
            break
        net_dict["lstm%i_pool" % i]["pool_size"] = (2,)
        f *= 2
    # Increase last pool-size to get the initial reduction factor.
    assert orig_red_factor % f == 0
    last_pool_size = orig_red_factor // f
    # Increase last pool-size to get the same encoder-seq-length folding.
    net_dict["lstm%i_pool" % (num_lstm_layers - 2)]["pool_size"] = (last_pool_size,)
    # Delete non-used lstm layers. This is not explicitly necessary but maybe nicer.
    for i in range(num_lstm_layers - 1, orig_num_lstm_layers - 1):
        del net_dict["lstm%i_fw" % i]
        del net_dict["lstm%i_bw" % i]
        del net_dict["lstm%i_pool" % i]
    return net_dict

#pretrain = {"repetitions": 5, "construction_algo": custom_construction_algo}

num_epochs = 120
model = "net-model/network"
cleanup_old_models = True
gradient_clip = 0
#gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
#debug_add_check_numerics_ops = True
debug_add_check_numerics_on_output = True
tf_log_memory_usage = True
gradient_noise = 0.0
learning_rate = 0.001
learning_rates = list(numpy.linspace(0.00002, 0.001, num=30))  # warmup
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.7
learning_rate_file = "newbob.data"

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5



