**Pre-trained models** can be downloaded from [here](http://www-i6.informatik.rwth-aachen.de/~irie/models/librispeech/2019-lm-transformers).

If you make use of these configs and/or models, please cite this paper:
```
@inproceedings {irie19:trafolm,
author= {Irie, Kazuki and Zeyer, Albert and Schl\"uter, Ralf and Ney, Hermann},	
title= {Language Modeling with Deep Transformers},	
booktitle= {Interspeech},	
year= 2019,	
address= {Graz, Austria},	
month= sep,	
url = {http://arxiv.org/pdf/1905.04226.pdf}
}
```
